---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I'm a PhD student at the [Centre for Speech Technology Research (CSTR)](https://www.cstr.ed.ac.uk), affiliated with the Institute for Language, Cognition and Computation (ILCC), University of Edinburgh. I'm advised by [Dr. Catherine Lai](https://homepages.inf.ed.ac.uk/clai/) and [Prof. Peter Bell](https://homepages.inf.ed.ac.uk/pbell1/), and fully funded by the School of Informatics. I was an Enrichment student at the Alan Turing Institute, and a research intern at Microsoft Research.

My research aims to advance spoken language technologies in real-world applications by bridging different but relevant domains such as linguistics & paralinguistics, speech & language, emotion & health, etc. In particular, my work focuses on problems that hinder the broader use of spoken language technologies in the wild.

Before starting my PhD, I used to research on affective computing and human-robot interaction at Honda Innovation Lab, [Hiroshi Ishiguro Lab ATR](http://www.geminoid.jp/en/index.html), and [Speech and Audio Processing Lab](http://sap.ist.i.kyoto-u.ac.jp/EN/).
I was supervised by [Prof. Tatsuya Kawahara](http://sap.ist.i.kyoto-u.ac.jp/members/kawahara/) (FIEEE) and co-supervised by [Prof. Nigel Ward](https://hb2504.utep.edu/Home/Profile?username=nigel) and [Prof. Carlos Ishi](http://www.irc.atr.jp/~carlos/).


# üî• News
- *01/2024*: Our special session **Responsible Speech Foundation Models** has been accepted by Interspeech 2024.
- *11/2023*: **[Microsoft FADTK(https://github.com/microsoft/fadtk)]**, a Frechet audio distance toolkit has been released, to which I contributed its speech models.
- *09/2023*: &nbsp;üéâ Received the **IEEE SPS Scholarship** from the IEEE Signal Processing Society.
- *09/2023*: &nbsp;üéâ Received the **Outstanding Paper Award** at the SAI workshop, ACII2023, MIT Media Lab.
- *06/2023*: One grant proposal (as Co-Investigator) **Development of A Human-Centric Elderly Driving Education System** has been accepted by the Inter-University Research Institute Corporation, Research Organization of Information and Systems (ROIS).
- *03/2023*: &nbsp;üéâ Received the **Gary Marsden Travel Award** from ACM SIGCHI.

# üëî Experience
**Teaching**
- *2023*
TA (Coursework marker), Automatic Speech Recognition, University of Edinburgh
TA (Tutor, demonstrator, and project marker), System Design Project, University of Edinburgh
- *2022*
TA (Coursework and exam marker), Machine Learning, University of Edinburgh


**Supervision**
- Layerwise Analysis of HuBERT Acoustic Word Embeddings in the Context of Speech Emotion Recognition
  Alexandra Saliba, MSc dissertation 2023/24 (Distinction), University of Edinburgh
- Hierarchical Cross-Modal Transformer and A Study of Cross-Modal Attention for Affective Computing
  Yaoting Wang, MSc dissertation 2022/23 (Distinction), University of Edinburgh
- A Cross-Domain Study of Crossmodal Attention Based Multimodal Emotion Recognition
  Junling Liu, MSc dissertation 2021/22, University of Edinburgh


# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2016</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)

**Kaiming He**, Xiangyu Zhang, Shaoqing Ren, Jian Sun

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>

- [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020**

# üéñ Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# üìñ Educations
- *2019.06 - 2022.04 (now)*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2015.09 - 2019.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# üí¨ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)

# üíª Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China.